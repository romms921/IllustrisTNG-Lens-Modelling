{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d246f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import glafic\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import psutil\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json  # ### NEW ### - For handling the restart file\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a5e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lens corresponding parameters (order preserved)\n",
    "# POW\n",
    "pow_params = ['$z_{s,fid}$', 'x', 'y', 'e', '$θ_{e}$', '$r_{Ein}$', '$\\gamma$ (PWI)']\n",
    "\n",
    "# SIE\n",
    "sie_params = ['$\\sigma$', 'x', 'y', 'e', '$θ_{e}$', '$r_{core}$', 'NaN']\n",
    "\n",
    "# NFW\n",
    "nfw_params = ['M', 'x', 'y', 'e', '$θ_{e}$', 'c or $r_{s}$', 'NaN']\n",
    "\n",
    "# EIN\n",
    "ein_params = ['M', 'x', 'y', 'e', '$θ_{e}$', 'c or $r_{s}$', r'$\\alpha_{e}$']\n",
    "\n",
    "# SHEAR \n",
    "shear_params = ['$z_{s,fid}$', 'x', 'y', '$\\gamma$', '$θ_{\\gamma}$', 'NaN', '$\\kappa$']\n",
    "\n",
    "# Sersic\n",
    "sersic_params = ['$M_{tot}$', 'x', 'y', 'e', '$θ_{e}$', '$r_{e}$', '$n$']\n",
    "\n",
    "# Cored SIE\n",
    "cored_sie_params = ['M', 'x', 'y', 'e', '$θ_{e}$', '$r_{core}$', 'NaN']\n",
    "\n",
    "# Multipoles\n",
    "mpole_params = ['$z_{s,fid}$', 'x', 'y', '$\\epsilon$', '$θ_{m}$', 'm', 'n']\n",
    "\n",
    "model_list = ['POW', 'SIE', 'ANFW', 'EIN', 'PERT', 'SERS', 'MPOLE']\n",
    "model_params = {\n",
    "    'POW': pow_params,\n",
    "    'SIE': sie_params,\n",
    "    'ANFW': nfw_params,\n",
    "    'EIN': ein_params,\n",
    "    'PERT': shear_params,\n",
    "    'SERS': sersic_params,\n",
    "    'MPOLE' : mpole_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "base_path = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/'\n",
    "sim_version = 'Sim 7'\n",
    "\n",
    "m = [round(x, 3) for x in np.linspace(0.01, 0.5, 50)]\n",
    "n = [round(x, 1) for x in np.linspace(0, 360, 10)]\n",
    "o = [round(x, 2) for x in np.linspace(-0.5, 0.5, 10)]\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "num_files = (len(m) * len(n) * len(o)) // chunk_size\n",
    "\n",
    "# Read all files \n",
    "files = [f\"{base_path}{sim_version}_summary_{i+1}.csv\" for i in range(num_files)]\n",
    "print(f\"Reading {len(files)} files...\")\n",
    "\n",
    "model_name = 'POW'\n",
    "macro_model_params = model_name.strip().split('_')[0]\n",
    "macro_columns = model_params[macro_model_params]\n",
    "\n",
    "columns=['strength', 'pa', 'kappa', 'num_images', 'pos_rms', 'mag_rms', 't_shear_str', 't_shear_pa', 't_shear_kappa', 'chi2'] + macro_columns\n",
    "\n",
    "# Read individual files and check if all models are present \n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, names=columns)\n",
    "        if len(df) - 1 != chunk_size:\n",
    "            print(f\"Warning: {file} has {len(df) - 1} rows, expected {chunk_size}.\")\n",
    "            # Create tuples of (strength, pa, kappa) from current file\n",
    "            current_models = set(zip(df['strength'].astype(str), df['pa'].astype(str), df['kappa'].astype(str)))\n",
    "            # Create expected models for current chunk based on indices\n",
    "            current_idx = int(file.split('_')[-1].split('.')[0]) - 1\n",
    "            start_idx = current_idx * chunk_size\n",
    "            \n",
    "            # Calculate the product of m, n, o for the current chunk only\n",
    "            total_combinations = len(m) * len(n) * len(o)\n",
    "            if start_idx + chunk_size <= total_combinations:\n",
    "                combinations = list(product(m, n, o))[start_idx:start_idx + chunk_size]\n",
    "            else:\n",
    "                combinations = list(product(m, n, o))[start_idx:]\n",
    "                \n",
    "            expected_models = set(zip(map(str, [x[0] for x in combinations]), \n",
    "                                    map(str, [x[1] for x in combinations]), \n",
    "                                    map(str, [x[2] for x in combinations])))\n",
    "            missing_models = expected_models - current_models\n",
    "            if missing_models:\n",
    "                print(f\"Missing models in {file}: {missing_models}\")\n",
    "        else:\n",
    "            print(f\"{file} has the expected number of rows: {len(df) - 1}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73426bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 10 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_1.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_2.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_3.csv has 100001 rows, expected 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_4.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_5.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_6.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_7.csv has 100002 rows, expected 100000.\n",
      "Missing models in /Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_7.csv: {('0.07027', '360.0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_8.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_9.csv has the expected number of rows: 100000.\n",
      "/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 13_summary_10.csv has the expected number of rows: 100000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/964hg0yn70x0ddsqcj4bsf8r0000gn/T/ipykernel_56463/1087272226.py:26: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, names=columns)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "base_path = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/'\n",
    "sim_version = 'Sim 13'\n",
    "\n",
    "m = [round(x, 5) for x in np.linspace(0.001, 0.1, 1000)]\n",
    "n = [round(x, 5) for x in np.linspace(0, 360, 1000)]\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "num_files = (len(m) * len(n)) // chunk_size\n",
    "\n",
    "# Read all files \n",
    "files = [f\"{base_path}{sim_version}_summary_{i+1}.csv\" for i in range(num_files)]\n",
    "print(f\"Reading {len(files)} files...\")\n",
    "\n",
    "model_name = 'POW'\n",
    "macro_model_params = model_name.strip().split('_')[0]\n",
    "macro_columns = model_params[macro_model_params]\n",
    "\n",
    "columns=['strength', 'pa', 'num_images', 'pos_rms', 'mag_rms', 't_mpole_str', 't_mpole_pa', 'chi2'] + macro_columns\n",
    "\n",
    "# Read individual files and check if all models are present \n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, names=columns)\n",
    "        if len(df) - 1 != chunk_size:\n",
    "            print(f\"Warning: {file} has {len(df) - 1} rows, expected {chunk_size}.\")\n",
    "            # Create tuples of (strength, pa) from current file\n",
    "            current_models = set(zip(df['strength'].astype(str), df['pa'].astype(str)))\n",
    "            # Create expected models for current chunk based on indices\n",
    "            current_idx = int(file.split('_')[-1].split('.')[0]) - 1\n",
    "            start_idx = current_idx * chunk_size\n",
    "            \n",
    "            # Calculate the product of m, n for the current chunk only\n",
    "            total_combinations = len(m) * len(n)\n",
    "            if start_idx + chunk_size <= total_combinations:\n",
    "                combinations = list(product(m, n))[start_idx:start_idx + chunk_size]\n",
    "            else:\n",
    "                combinations = list(product(m, n))[start_idx:]\n",
    "\n",
    "            expected_models = set(zip(map(str, [x[0] for x in combinations]), \n",
    "                                    map(str, [x[1] for x in combinations])))\n",
    "            missing_models = expected_models - current_models\n",
    "            if missing_models:\n",
    "                print(f\"Missing models in {file}: {missing_models}\")\n",
    "        else:\n",
    "            print(f\"{file} has the expected number of rows: {len(df) - 1}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540a1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_extract(model_ver, model_path, constraint):\n",
    "    global pos_rms, mag_rms, chi2_value\n",
    "    # Load the data\n",
    "    with open(model_path + '/' + model_ver + '_optresult' + '.dat', 'r') as file:\n",
    "        opt_result = file.readlines()\n",
    "\n",
    "    # Find the last line with 'optimize' in it\n",
    "    last_optimize_index = None\n",
    "    for idx in range(len(opt_result) - 1, -1, -1):\n",
    "        if 'optimize' in opt_result[idx]:\n",
    "            last_optimize_index = idx\n",
    "            last_optimize_line = opt_result[idx]\n",
    "            break\n",
    "    if last_optimize_index is None:\n",
    "        raise ValueError(\"No line with 'optimize' found in the file.\")\n",
    "\n",
    "    # Extract everything after the last 'optimize' line\n",
    "    opt_result = opt_result[last_optimize_index + 1:]\n",
    "\n",
    "    # Count the number of lines that start with 'lens'\n",
    "    lens_count = sum(1 for line in opt_result if line.startswith('lens'))\n",
    "\n",
    "    # Initialize a dictionary to hold the lens parameters\n",
    "    lens_params_dict = {}\n",
    "\n",
    "    # Extract the lens parameters\n",
    "    lens_params = []\n",
    "    for line in opt_result:\n",
    "        if line.startswith('lens'):\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            lens_name = parts[1]\n",
    "            params = [float(x) for x in parts[2:]]\n",
    "\n",
    "            # Store the parameters in the dictionary\n",
    "            lens_params_dict[lens_name] = params\n",
    "            lens_params.append((lens_name, params))\n",
    "\n",
    "    # Remove the first lens parameter\n",
    "    if lens_params:\n",
    "        for i in range(len(lens_params)):\n",
    "            lens_name, params = lens_params[i]\n",
    "            lens_params_dict[lens_name] = params[1:]\n",
    "    \n",
    "    # Extract the chi2 \n",
    "    chi2_line = next((line for line in opt_result if 'chi^2' in line), None)\n",
    "    if chi2_line is None:\n",
    "        raise ValueError(\"No line with 'chi2' found in the file.\")\n",
    "\n",
    "    chi2_value = float(chi2_line.split('=')[-1].strip().split()[0])\n",
    "    print(f\"✅ Extracted chi2 value: {chi2_value}\")\n",
    "\n",
    "    # Number of len profiles\n",
    "    num_lens_profiles = len(lens_params_dict)\n",
    "\n",
    "    # Use generic column names: param1, param2, ...\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    max_param_len = 0\n",
    "\n",
    "    for lens_name, params in lens_params_dict.items():\n",
    "        row = {'Lens Name': lens_name}\n",
    "        for i, val in enumerate(params):\n",
    "            row[f'param{i+1}'] = val\n",
    "        rows.append(row)\n",
    "        if len(params) > max_param_len:\n",
    "            max_param_len = len(params)\n",
    "\n",
    "    columns = ['Lens Name'] + [f'param{i+1}' for i in range(max_param_len)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    # Load the input parameters from the Python file\n",
    "    with open('/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/POS+MAG/SIE+SHEAR/pos_point.py', 'r') as file:\n",
    "        py = file.readlines()\n",
    "\n",
    "    # Extracting the input parameters from the Python file\n",
    "    set_lens_lines = [line for line in py if line.startswith('glafic.set_lens(')]\n",
    "    if not set_lens_lines:\n",
    "        raise ValueError(\"No lines starting with 'glafic.set_lens(' found in the file.\")\n",
    "\n",
    "    set_lens_params = []\n",
    "    for line in set_lens_lines:\n",
    "        match = re.search(r'set_lens\\((.*?)\\)', line)\n",
    "        if match:\n",
    "            params_str = match.group(1)\n",
    "            params = [param.strip() for param in params_str.split(',')]\n",
    "            set_lens_params.append(params)\n",
    "        else:\n",
    "            raise ValueError(f\"No valid parameters found in line: {line.strip()}\")\n",
    "\n",
    "    # Store the parameters in a dictionary\n",
    "    set_lens_dict = {}\n",
    "    for params in set_lens_params:\n",
    "        if len(params) < 3:\n",
    "            raise ValueError(f\"Not enough parameters found in line: {params}\")\n",
    "        lens_name = params[1].strip(\"'\\\"\")  # Remove quotes from lens name\n",
    "        lens_params = [float(x) for x in params[2:]]  # Skip index and lens name\n",
    "        set_lens_dict[lens_name] = lens_params\n",
    "\n",
    "    # Remove the first lens parameter\n",
    "    if set_lens_dict:\n",
    "        for lens_name, params in set_lens_dict.items():\n",
    "            set_lens_dict[lens_name] = params[1:]  # Remove the first parameter (index)\n",
    "\n",
    "    # Use generic column names: param1, param2, ...\n",
    "    df_input = pd.DataFrame()\n",
    "    rows_input = []\n",
    "    max_param_len_input = 0\n",
    "    for lens_name, params in set_lens_dict.items():\n",
    "        row = {'Lens Name': lens_name}\n",
    "        for i, val in enumerate(params):\n",
    "            row[f'param{i+1}'] = val\n",
    "        rows_input.append(row)\n",
    "        if len(params) > max_param_len_input:\n",
    "            max_param_len_input = len(params)\n",
    "    columns_input = ['Lens Name'] + [f'param{i+1}' for i in range(max_param_len_input)]\n",
    "    df_input = pd.DataFrame(rows_input, columns=columns_input)\n",
    "    \n",
    "    # Extract input flags from the Python file\n",
    "    set_flag_lines = [line for line in py if line.startswith('glafic.setopt_lens(')]\n",
    "    if not set_flag_lines:\n",
    "        raise ValueError(\"No lines starting with 'glafic.setopt_lens(' found in the file.\")\n",
    "    set_flag_params = []\n",
    "    for line in set_flag_lines:\n",
    "        match = re.search(r'setopt_lens\\((.*?)\\)', line)\n",
    "        if match:\n",
    "            params_str = match.group(1)\n",
    "            params = [param.strip() for param in params_str.split(',')]\n",
    "            set_flag_params.append(params)\n",
    "        else:\n",
    "            raise ValueError(f\"No valid parameters found in line: {line.strip()}\")\n",
    "    \n",
    "    # Store the parameters in a dictionary\n",
    "    set_flag_dict = {}\n",
    "    for params in set_flag_params:\n",
    "        if len(params) < 2:\n",
    "            raise ValueError(f\"Not enough parameters found in line: {params}\")\n",
    "        # The lens name is not present in setopt_lens, so use the lens index to map to set_lens_dict\n",
    "        lens_index = params[0].strip(\"'\\\"\")\n",
    "        # Find the lens name corresponding to this index from set_lens_params\n",
    "        lens_name = None\n",
    "        for lens_params in set_lens_params:\n",
    "            if lens_params[0].strip(\"'\\\"\") == lens_index:\n",
    "                lens_name = lens_params[1].strip(\"'\\\"\")\n",
    "                break\n",
    "        if lens_name is None:\n",
    "            raise ValueError(f\"Lens name for index {lens_index} not found in set_lens_params\")\n",
    "        flag = ','.join(params[1:])  # Join all flag values as a string\n",
    "        set_flag_dict[lens_name] = flag\n",
    "   \n",
    "    # Remove the first flag parameter\n",
    "    if set_flag_dict:\n",
    "        for lens_name, flag in set_flag_dict.items():\n",
    "            flag_parts = flag.split(',')\n",
    "            set_flag_dict[lens_name] = ','.join(flag_parts[1:])  # Remove the first flag parameter\n",
    "    \n",
    "    # Dynamically create columns: 'Lens Name', 'flag1', 'flag2', ..., based on the maximum number of flags\n",
    "    df_flag = pd.DataFrame()\n",
    "    rows_flag = []\n",
    "    max_flag_len = 0\n",
    "    \n",
    "    # First, determine the maximum number of flags\n",
    "    for flag in set_flag_dict.values():\n",
    "        flag_parts = flag.split(',')\n",
    "        if len(flag_parts) > max_flag_len:\n",
    "            max_flag_len = len(flag_parts)\n",
    "    for lens_name, flag in set_flag_dict.items():\n",
    "        flag_parts = flag.split(',')\n",
    "        row = {'Lens Name': lens_name}\n",
    "        for i, val in enumerate(flag_parts):\n",
    "            row[f'flag{i+1}'] = val\n",
    "        rows_flag.append(row)\n",
    "    columns_flag = ['Lens Name'] + [f'flag{i+1}' for i in range(max_flag_len)]  \n",
    "    df_flag = pd.DataFrame(rows_flag, columns=columns_flag)\n",
    "    \n",
    "    # Combine all dataframes into a list of dataframes for each lens\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(num_lens_profiles):\n",
    "        lens_name = df['Lens Name'][i]\n",
    "        \n",
    "        # Find the model type (case-insensitive match)\n",
    "        model_type = None\n",
    "        for m in model_list:\n",
    "            if m.lower() == lens_name.lower():\n",
    "                model_type = m\n",
    "                break\n",
    "        if model_type is None:\n",
    "            continue\n",
    "\n",
    "        symbols = model_params[model_type][:7]\n",
    "        # Row 2: input\n",
    "        row_input = pd.DataFrame([df_input.iloc[i, 1:8].values], columns=symbols)\n",
    "        # Row 3: output\n",
    "        row_output = pd.DataFrame([df.iloc[i, 1:8].values], columns=symbols)\n",
    "        # Row 4: flags\n",
    "        row_flags = pd.DataFrame([df_flag.iloc[i, 1:8].values], columns=symbols)\n",
    "\n",
    "        # Stack vertically, add a label column for row type\n",
    "        lens_df = pd.concat([\n",
    "            row_input.assign(Type='Input'),\n",
    "            row_output.assign(Type='Output'),\n",
    "            row_flags.assign(Type='Flag')\n",
    "        ], ignore_index=True)\n",
    "        lens_df.insert(0, 'Lens Name', lens_name)\n",
    "        \n",
    "        # Move 'Type' to the second column\n",
    "        cols = lens_df.columns.tolist()\n",
    "        cols.insert(1, cols.pop(cols.index('Type')))\n",
    "        lens_df = lens_df[cols]\n",
    "        dfs.append(lens_df)\n",
    "    \n",
    "    # Anomaly Calculation\n",
    "    columnn_names = ['x', 'y', 'mag', 'pos_err', 'mag_err', '1', '2', '3']\n",
    "    obs_point = pd.read_csv('/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/obs_point/obs_point_(POS+FLUX).dat', delim_whitespace=True, header=None, skiprows=1, names=columnn_names)\n",
    "    out_point = pd.read_csv(model_path + '/' + model_ver + '_point.dat', delim_whitespace=True, header=None, skiprows=1, names=columnn_names)\n",
    "    out_point.drop(columns=['mag_err', '1', '2', '3'], inplace=True)\n",
    "\n",
    "    # Drop rows in obs_point where the corresponding out_point['mag'] < 1\n",
    "    mask = abs(out_point['mag']) >= 1\n",
    "    out_point = out_point[mask[:len(out_point)]].reset_index(drop=True)\n",
    "    out_point['x_diff'] = abs(out_point['x'] - obs_point['x'])\n",
    "    out_point['y_diff'] = abs(out_point['y'] - obs_point['y'])\n",
    "    out_point['mag_diff'] = abs(abs(out_point['mag']) - abs(obs_point['mag']))\n",
    "    out_point['pos_sq'] = np.sqrt((out_point['x_diff']**2 + out_point['y_diff']**2).astype(float))  # Plotted on graph\n",
    "\n",
    "    # RMS\n",
    "    pos_rms = np.average(out_point['pos_sq'])\n",
    "\n",
    "    mag_rms = np.average(np.sqrt((out_point['mag_diff']**2).astype(float)))\n",
    "\n",
    "    return pos_rms, mag_rms, dfs, chi2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6103908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = base_path\n",
    "\n",
    "for strength, pa, kappa in missing_models:\n",
    "    model_name = f'SIE_POS_SHEAR_{strength}_{pa}_{kappa}'\n",
    "    model_path = os.path.join(model_output_dir, model_name)\n",
    "\n",
    "    print(f\"\\nProcessing missing model: {model_name}\")\n",
    "\n",
    "    # --- Model Generation ---\n",
    "    glafic.init(0.3, 0.7, -1.0, 0.7, '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left', 20.0, 20.0, 21.56, 21.56, 0.01, 0.01, 1, verb=0)\n",
    "    glafic.set_secondary('chi2_splane 1', verb=0)\n",
    "    glafic.set_secondary('chi2_checknimg 1', verb=0)\n",
    "    glafic.set_secondary('chi2_restart   -1', verb=0)\n",
    "    glafic.set_secondary('chi2_usemag    1', verb=0)\n",
    "    glafic.set_secondary('hvary          0', verb=0)\n",
    "    glafic.set_secondary('ran_seed -122000', verb=0)\n",
    "    glafic.startup_setnum(2, 0, 1)\n",
    "    glafic.set_lens(1, 'sie', 0.261343256161012, 1.549839e+02, 20.78, 20.78, 0.107, 23.38, 0.0, 0.0)\n",
    "    glafic.set_lens(2, 'pert', 0.261343256161012, 1.0, 20.78, 20.78, float(strength), float(pa), 0.0, float(kappa))\n",
    "    glafic.set_point(1, 1.0, 20.78, 20.78)\n",
    "    glafic.setopt_lens(1, 0, 1, 1, 1, 1, 1, 0, 0)\n",
    "    glafic.setopt_lens(2, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "    glafic.setopt_point(1, 0, 1, 1)\n",
    "    glafic.model_init(verb=0)\n",
    "    glafic.readobs_point('/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/obs_point/obs_point_(POS).dat')\n",
    "    glafic.optimize()\n",
    "    glafic.findimg()\n",
    "    glafic.writecrit(1.0)\n",
    "    glafic.writelens(1.0)\n",
    "    glafic.quit()\n",
    "\n",
    "    columns = ['x', 'y', 'm', 'm_err']\n",
    "    file_name = 'left_point.dat'\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        data = pd.read_csv(file_name, delim_whitespace=True, skiprows=1, header=None, names=columns)\n",
    "        num_images = len(data)\n",
    "        \n",
    "        constraint = 'pos'  # Since model name contains 'POS'\n",
    "        pos_rms, mag_rms, dfs, chi2 = rms_extract('left', '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test', constraint)\n",
    "\n",
    "        # Create the result dataframe\n",
    "        result_df = pd.DataFrame({\n",
    "            'strength': [strength],\n",
    "            'pa': [pa],\n",
    "            'kappa': [kappa],\n",
    "            'num_images': [num_images],\n",
    "            'pos_rms': [pos_rms],\n",
    "            'mag_rms': [mag_rms],\n",
    "            't_shear_str': [dfs[1]['$\\gamma$'][1]],\n",
    "            't_shear_pa': [dfs[1]['$θ_{\\gamma}$'][1]],\n",
    "            't_shear_kappa': [dfs[1]['$\\kappa$'][1]],\n",
    "            'sie_vel_disp': [dfs[0]['$\\sigma$'][1]],\n",
    "            'sie_pa': [dfs[0]['$θ_{e}$'][1]],\n",
    "            'sie_ell': [dfs[0]['e'][1]],\n",
    "            'chi2': [chi2]\n",
    "        })\n",
    "\n",
    "        # Find the appropriate chunk file based on the model parameters\n",
    "        param_idx = list(product(m, n, o)).index((float(strength), float(pa), float(kappa)))\n",
    "        chunk_num = param_idx // chunk_size + 1\n",
    "        chunk_file = f'{base_path}{sim_version}_summary_{chunk_num}.csv'\n",
    "        \n",
    "        # Read existing data\n",
    "        existing_df = pd.read_csv(chunk_file)\n",
    "        \n",
    "        # Append new data\n",
    "        updated_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "        \n",
    "        # Save back to CSV\n",
    "        updated_df.to_csv(chunk_file, index=False)\n",
    "        \n",
    "        print(f\"Results appended to {chunk_file}\")\n",
    "        \n",
    "        # Clean up generated files\n",
    "        crit_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_crit.dat'\n",
    "        lens_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_lens.fits'\n",
    "        point_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_point.dat'\n",
    "        opt_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_optresult.dat'\n",
    "\n",
    "        for file_to_delete in [crit_file, lens_file, point_file, opt_file]:\n",
    "            if os.path.exists(file_to_delete):\n",
    "                os.remove(file_to_delete)\n",
    "    else:\n",
    "        print(f\"File {file_name} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098f6b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmissing_models\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'missing_models' is not defined"
     ]
    }
   ],
   "source": [
    "missing_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de306cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = base_path\n",
    "\n",
    "for strength, pa in missing_models:\n",
    "    model_name = f'POW_POS_MPOLE_{strength}_{pa}'\n",
    "    model_path = os.path.join(model_output_dir, model_name)\n",
    "\n",
    "    print(f\"\\nProcessing missing model: {model_name}\")\n",
    "\n",
    "    # --- Model Generation ---\n",
    "    glafic.init(0.3, 0.7, -1.0, 0.7, '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left', 20.0, 20.0, 21.56, 21.56, 0.01, 0.01, 1, verb=0)\n",
    "    glafic.set_secondary('chi2_splane 1', verb=0)\n",
    "    glafic.set_secondary('chi2_checknimg 0', verb=0)\n",
    "    glafic.set_secondary('chi2_restart   -1', verb=0)\n",
    "    glafic.set_secondary('chi2_usemag    1', verb=0)\n",
    "    glafic.set_secondary('hvary          0', verb=0)\n",
    "    glafic.set_secondary('ran_seed -122000', verb=0)\n",
    "    glafic.startup_setnum(2, 0, 1)\n",
    "    glafic.set_lens(1, 'pow', 0.261343256161012, 1.0, 20.78, 20.78, 0.107, 23.38, 0.46, 2.1)\n",
    "    glafic.set_lens(2, 'mpole', 0.261343256161012, 1.0, 20.78, 20.78, float(strength), float(pa), 1.0, 1.0)\n",
    "    glafic.set_point(1, 1.0, 20.78, 20.78)\n",
    "    glafic.setopt_lens(1, 0, 0, 1, 1, 1, 1, 1, 1)\n",
    "    glafic.setopt_lens(2, 0, 0, 1, 1, 0, 0, 0, 1)\n",
    "    glafic.setopt_point(1, 0, 1, 1)\n",
    "    glafic.model_init(verb=0)\n",
    "    glafic.readobs_point('/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/obs_point/obs_point_(POS).dat')\n",
    "    glafic.parprior('/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/MPOLE/priorfile.dat')\n",
    "    glafic.optimize()\n",
    "    glafic.findimg()\n",
    "    # glafic.writecrit(1.0)\n",
    "    # glafic.writelens(1.0)\n",
    "    glafic.quit()\n",
    "\n",
    "    columns = ['x', 'y', 'm', 'm_err']\n",
    "    file_name = 'left_point.dat'\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        data = pd.read_csv(file_name, delim_whitespace=True, skiprows=1, header=None, names=columns)\n",
    "        num_images = len(data)\n",
    "        \n",
    "        constraint = 'pos'  # Since model name contains 'POS'\n",
    "        pos_rms, mag_rms, dfs, chi2 = rms_extract('left', '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test', constraint)\n",
    "\n",
    "        macro_model_params = model_name.strip().split('_')[0]\n",
    "        macro_columns = model_params[macro_model_params]\n",
    "\n",
    "        # Create the result dataframe\n",
    "        result_df = pd.DataFrame({\n",
    "            'strength': [strength],\n",
    "            'pa': [pa],\n",
    "            'num_images': [num_images],\n",
    "            'pos_rms': [pos_rms],\n",
    "            'mag_rms': [mag_rms],\n",
    "            't_mpole_str': [dfs[1]['$\\epsilon$'][1]],\n",
    "            't_mpole_pa': [dfs[1]['$θ_{m}$'][1]],\n",
    "            'chi2': [chi2],\n",
    "            **{col: [dfs[0][col][1]] for col in macro_columns}\n",
    "        })\n",
    "\n",
    "        # Find the appropriate chunk file based on the model parameters\n",
    "        param_idx = list(product(m, n)).index((float(strength), float(pa)))\n",
    "        chunk_num = param_idx // chunk_size + 1\n",
    "        chunk_file = f'{base_path}{sim_version}_summary_{chunk_num}.csv'\n",
    "        \n",
    "        # Read existing data\n",
    "        existing_df = pd.read_csv(chunk_file)\n",
    "        \n",
    "        # Append new data\n",
    "        updated_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "        \n",
    "        # Save back to CSV\n",
    "        updated_df.to_csv(chunk_file, index=False)\n",
    "        \n",
    "        print(f\"Results appended to {chunk_file}\")\n",
    "        \n",
    "        # Clean up generated files\n",
    "        # crit_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_crit.dat'\n",
    "        # lens_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_lens.fits'\n",
    "        point_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_point.dat'\n",
    "        opt_file = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/left_optresult.dat'\n",
    "\n",
    "        for file_to_delete in [point_file, opt_file]:\n",
    "            if os.path.exists(file_to_delete):\n",
    "                os.remove(file_to_delete)\n",
    "    else:\n",
    "        print(f\"File {file_name} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb896055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all dataframes together\n",
    "df = pd.concat((pd.read_csv(file, names=columns, skiprows=1) for file in files), ignore_index=True)\n",
    "\n",
    "df.to_csv(f'{base_path}{sim_version}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_csv = '/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling/Test/Sim 11_summary_208.csv'\n",
    "problem_data = pd.read_csv(problem_csv, header=0)\n",
    "problem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b26b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for repeated combinations of strength and pa\n",
    "repeated_combinations = problem_data[problem_data.duplicated(subset=['strength', 'pa'], keep=False)]\n",
    "repeated_combinations\n",
    "print(repeated_combinations)\n",
    "\n",
    "# Keep only one set of repeated combinations in original csv\n",
    "cleaned_problem_data = problem_data.drop_duplicates(subset=['strength', 'pa'], keep='first')\n",
    "cleaned_problem_data.to_csv(problem_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
