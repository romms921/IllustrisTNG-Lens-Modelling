{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e4779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/ainsleylewis/Documents/Astronomy/IllustrisTNG Lens Modelling\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b1d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "from error_propagation import Complex\n",
    "from astropy.io import fits\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.patches import Ellipse\n",
    "from astropy.visualization import SqrtStretch\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.psf import PSFPhotometry, IterativePSFPhotometry\n",
    "from photutils.psf import make_psf_model_image, CircularGaussianPSF, GaussianPSF, MoffatPSF\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import emcee\n",
    "norm = ImageNormalize(stretch=SqrtStretch())\n",
    "\n",
    "# Galfic Plotting Code\n",
    "from astropy.io import fits\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Glafic Tabular \n",
    "\n",
    "# Read_script.py\n",
    "# Open the Python file as a text file\n",
    "def glafic_tabular(filename_0, filename_6, save_table_flag = False, shear = False, show_shear = False, show_params = False):\n",
    "    with open(filename_0, 'r') as file:\n",
    "        content = file.read()\n",
    "        file.close()\n",
    "    \n",
    "    with open(filename_6, 'r') as file:\n",
    "        content_opt = file.read()\n",
    "        file.close()\n",
    "\n",
    "    def find_line(word, content):\n",
    "        content = content.split('\\n')\n",
    "        line_number = 0\n",
    "        for line in content:\n",
    "            line_number += 1\n",
    "            if word in line:\n",
    "                return line_number\n",
    "        return \"Line Not Found\"\n",
    "    \n",
    "    def find_last_line(word, content):\n",
    "        content = content.split('\\n')\n",
    "        line_number = 0\n",
    "        last_occurrence = -1\n",
    "        for line in content:\n",
    "            line_number += 1\n",
    "            if word in line:\n",
    "                last_occurrence = line_number\n",
    "        return last_occurrence\n",
    "\n",
    "    content_list = content.split('\\n')\n",
    "\n",
    "    if shear == True:\n",
    "        # --- FIX STARTS HERE ---\n",
    "        # Instead of hardcoding '2', we will find the 'pert' lens dynamically.\n",
    "        pert_lens_number = None\n",
    "        set_lens_pert_line_content = None\n",
    "\n",
    "        for line in content_list:\n",
    "            # Find the line that defines the 'pert' lens model\n",
    "            if 'glafic.set_lens(' in line and \"'pert'\" in line:\n",
    "                set_lens_pert_line_content = line\n",
    "                # Extract the lens number from the line \"glafic.set_lens(N, 'pert', ...)\"\n",
    "                try:\n",
    "                    # Split by '(' and then by ',' to isolate the number\n",
    "                    pert_lens_number = int(line.split('(')[1].split(',')[0].strip())\n",
    "                except (ValueError, IndexError):\n",
    "                    raise ValueError(f\"Could not parse lens number from 'pert' line: {line}\")\n",
    "                break # Stop after finding the first one\n",
    "\n",
    "        # If no 'pert' lens was found in the entire file\n",
    "        if pert_lens_number is None:\n",
    "            raise ValueError(\"Shear mode is on, but no lens with model 'pert' was found in the file.\")\n",
    "\n",
    "        # Now, dynamically find the corresponding setopt_lens line using the number we found\n",
    "        setopt_search_string = f'glafic.setopt_lens({pert_lens_number},'\n",
    "        line_opt_pert_num = find_line(setopt_search_string, content)\n",
    "\n",
    "        if line_opt_pert_num == \"Line Not Found\":\n",
    "            raise ValueError(f\"Found 'pert' lens {pert_lens_number}, but could not find its corresponding 'setopt_lens' line.\")\n",
    "        \n",
    "        # Get the content of the setopt line\n",
    "        setopt_lens_pert_line_content = content_list[line_opt_pert_num - 1]\n",
    "        \n",
    "        # The rest of the logic is the same, just using our dynamically found lines\n",
    "        parts_set_lens_2 = set_lens_pert_line_content.split(',')\n",
    "        parts_set_lens_2 = [part.strip().strip(\"'\") for part in parts_set_lens_2]\n",
    "\n",
    "        parts_setopt_lens_2 = setopt_lens_pert_line_content.split(',')\n",
    "        parts_setopt_lens_2 = [part.strip().strip(\"'\") for part in parts_setopt_lens_2]\n",
    "\n",
    "        name = 'Shear'\n",
    "        z_s = parts_set_lens_2[3]\n",
    "        x = parts_set_lens_2[4]\n",
    "        y = parts_set_lens_2[5]\n",
    "        gamma = parts_set_lens_2[6]\n",
    "        pa = parts_set_lens_2[7]\n",
    "        NaN = parts_set_lens_2[8]\n",
    "        pwi = parts_set_lens_2[9].replace(')', '') \n",
    "\n",
    "        z_s_flag = parts_setopt_lens_2[2]\n",
    "        x_flag = parts_setopt_lens_2[3]\n",
    "        y_flag = parts_setopt_lens_2[4]\n",
    "        gamma_flag = parts_setopt_lens_2[5]\n",
    "        pa_flag = parts_setopt_lens_2[6]\n",
    "        NaN_flag = parts_setopt_lens_2[7]\n",
    "        pwi_flag = parts_setopt_lens_2[8].replace(')', '') \n",
    "\n",
    "        line = find_last_line('lens   pert', content_opt)\n",
    "        line_content = content_opt.split('\\n')[line - 1]\n",
    "        line_list = line_content.split()\n",
    "\n",
    "        row_0 = [name, '$z_{s,fid}$', 'x', 'y', 'γ', '$θ_{γ}$', 'NaN', 'κ']\n",
    "        row_1 = ['Input Value', z_s, x, y, gamma, pa, NaN, pwi]\n",
    "        row_2 = ['Opt Result', line_list[3], line_list[4], line_list[5], line_list[6], line_list[7], line_list[8], line_list[9]]\n",
    "        row_3 = ['Fixed', z_s_flag, x_flag, y_flag, gamma_flag, pa_flag, NaN_flag, pwi_flag]\n",
    "\n",
    "        table_shear = pd.DataFrame([row_1, row_2, row_3], columns = row_0)\n",
    "        if show_shear:\n",
    "            print(table_shear)\n",
    "\n",
    "        if save_table_flag:\n",
    "            table_shear.to_csv('shear_table.csv')\n",
    "            \n",
    "        return table_shear\n",
    "        # --- FIX ENDS HERE ---\n",
    "\n",
    "    # --- This is the non-shear part of the function, it remains unchanged ---\n",
    "    line_set_1 = find_line('glafic.set_lens(1,', content)\n",
    "    line_opt_1 = find_line('glafic.setopt_lens(1,', content)\n",
    "\n",
    "    if line_set_1 == \"Line Not Found\" or line_opt_1 == \"Line Not Found\":\n",
    "        raise ValueError(\"Failed to find lens or setopt lens lines in the glafic file.\")\n",
    "    \n",
    "    set_lens_1 = content_list[line_set_1-1]\n",
    "    setopt_lens_1 = content_list[line_opt_1-1]\n",
    "    \n",
    "    models = ['SIE', 'POW', 'NFW', 'EIN']\n",
    "\n",
    "    parts_set_lens = set_lens_1.split(',')\n",
    "    parts_set_lens = [part.strip().strip(\"'\") for part in parts_set_lens]\n",
    "\n",
    "    parts_setopt_lens = setopt_lens_1.split(',')\n",
    "    parts_setopt_lens = [part.strip().strip(\"'\") for part in parts_setopt_lens]\n",
    "    \n",
    "    for i in models:\n",
    "        i = i.lower()\n",
    "        if i in set_lens_1:\n",
    "            if i == models[1].lower(): # POW model\n",
    "                name = models[1]\n",
    "                z_s = parts_set_lens[3]\n",
    "                x = parts_set_lens[4]\n",
    "                y = parts_set_lens[5]\n",
    "                e = parts_set_lens[6]\n",
    "                pa = parts_set_lens[7]\n",
    "                r_ein = parts_set_lens[8]\n",
    "                pwi = parts_set_lens[9].replace(')', '') \n",
    "\n",
    "                z_s_flag = parts_setopt_lens[2]\n",
    "                x_flag = parts_setopt_lens[3]\n",
    "                y_flag = parts_setopt_lens[4]\n",
    "                e_flag = parts_setopt_lens[5]\n",
    "                pa_flag = parts_setopt_lens[6]\n",
    "                r_ein_flag = parts_setopt_lens[7]\n",
    "                pwi_flag = parts_setopt_lens[8].replace(')', '') \n",
    "\n",
    "                line = find_last_line('lens   pow', content_opt)\n",
    "                line_content = content_opt.split('\\n')[line - 1]\n",
    "                line_list = line_content.split()\n",
    "\n",
    "                chi = find_last_line('chi^2', content_opt)\n",
    "                chi_content = content_opt.split('\\n')[chi - 1]\n",
    "                chi_list = chi_content.split()\n",
    "\n",
    "                row_0 = [name, '$z_{s,fid}$', 'x', 'y', 'e', '$θ_{e}$', '$r_{Ein}$', 'γ (PWI)']\n",
    "                row_1 = ['Input Value', z_s, x, y, e, pa, r_ein, pwi]\n",
    "                row_2 = ['Opt Result', line_list[3], line_list[4], line_list[5], line_list[6], line_list[7], line_list[8], line_list[9]]\n",
    "                row_3 = ['Fixed', z_s_flag, x_flag, y_flag, e_flag, pa_flag, r_ein_flag, pwi_flag]\n",
    "\n",
    "                table = pd.DataFrame([row_1, row_2, row_3], columns = row_0)\n",
    "\n",
    "            elif i == models[0].lower(): # SIE model\n",
    "                name = models[0]\n",
    "                sigma = parts_set_lens[3]\n",
    "                x = parts_set_lens[4]\n",
    "                y = parts_set_lens[5]\n",
    "                e = parts_set_lens[6]\n",
    "                pa = parts_set_lens[7]\n",
    "                r_core = parts_set_lens[8]\n",
    "                pwi = parts_set_lens[9].replace(')', '') \n",
    "\n",
    "                sigma_flag = parts_setopt_lens[2]\n",
    "                x_flag = parts_setopt_lens[3]\n",
    "                y_flag = parts_setopt_lens[4]\n",
    "                e_flag = parts_setopt_lens[5]\n",
    "                pa_flag = parts_setopt_lens[6]\n",
    "                r_core_flag = parts_setopt_lens[7]\n",
    "                pwi_flag = parts_setopt_lens[8].replace(')', '') \n",
    "\n",
    "                line = find_last_line('lens   sie', content_opt)\n",
    "                line_content = content_opt.split('\\n')[line - 1]\n",
    "                line_list = line_content.split()\n",
    "\n",
    "                chi = find_last_line('chi^2', content_opt)\n",
    "                chi_content = content_opt.split('\\n')[chi - 1]\n",
    "                chi_list = chi_content.split()\n",
    "\n",
    "                row_0 = [name, 'σ', 'x', 'y', 'e', '$θ_{e}$', '$r_{core}$', 'NaN']\n",
    "                row_1 = ['Input Value', sigma, x, y, e, pa, r_core, pwi]\n",
    "                row_2 = ['Opt Result', line_list[3], line_list[4], line_list[5], line_list[6], line_list[7], line_list[8], line_list[9]]\n",
    "                row_3 = ['Fixed', sigma_flag, x_flag, y_flag, e_flag, pa_flag, r_core_flag, pwi_flag]\n",
    "\n",
    "                table = pd.DataFrame([row_1, row_2, row_3], columns = row_0)\n",
    "\n",
    "            elif i == models[2].lower(): # NFW model    \n",
    "                name = models[2]\n",
    "                m = parts_set_lens[3]\n",
    "                x = parts_set_lens[4]\n",
    "                y = parts_set_lens[5]\n",
    "                e = parts_set_lens[6]\n",
    "                pa = parts_set_lens[7]\n",
    "                c = parts_set_lens[8]\n",
    "                Nan = parts_set_lens[9].replace(')', '') \n",
    "\n",
    "                m_flag = parts_setopt_lens[2]\n",
    "                x_flag = parts_setopt_lens[3]\n",
    "                y_flag = parts_setopt_lens[4]\n",
    "                e_flag = parts_setopt_lens[5]\n",
    "                pa_flag = parts_setopt_lens[6]\n",
    "                c_flag = parts_setopt_lens[7]\n",
    "                Nan_flag = parts_setopt_lens[8].replace(')', '')\n",
    "\n",
    "                line = find_last_line('lens   anfw', content_opt)\n",
    "                line_content = content_opt.split('\\n')[line - 1]\n",
    "                line_list = line_content.split()\n",
    "\n",
    "                chi = find_last_line('chi^2', content_opt)\n",
    "                chi_content = content_opt.split('\\n')[chi - 1]\n",
    "                chi_list = chi_content.split()\n",
    "\n",
    "                row_0 = [name, 'M', 'x', 'y', 'e', '$θ_{e}$', 'c or $r_{s}$', 'NaN']\n",
    "                row_1 = ['Input Value', m, x, y, e, pa, c, Nan]\n",
    "                row_2 = ['Opt Result', line_list[3], line_list[4], line_list[5], line_list[6], line_list[7], line_list[8], line_list[9]]\n",
    "                row_3 = ['Fixed', m_flag, x_flag, y_flag, e_flag, pa_flag, c_flag, Nan_flag]\n",
    "\n",
    "                table = pd.DataFrame([row_1, row_2, row_3], columns = row_0)\n",
    "            \n",
    "            elif i == models[3].lower(): # EIN model\n",
    "                name = models[3]\n",
    "                m = parts_set_lens[3]\n",
    "                x = parts_set_lens[4]\n",
    "                y = parts_set_lens[5]\n",
    "                e = parts_set_lens[6]\n",
    "                pa = parts_set_lens[7]\n",
    "                ce_rs = parts_set_lens[8]\n",
    "                alpha_e = parts_set_lens[9].replace(')', '')\n",
    "\n",
    "                m_flag = parts_setopt_lens[2]\n",
    "                x_flag = parts_setopt_lens[3]\n",
    "                y_flag = parts_setopt_lens[4]\n",
    "                e_flag = parts_setopt_lens[5]\n",
    "                pa_flag = parts_setopt_lens[6]\n",
    "                ce_rs_flag = parts_setopt_lens[7]\n",
    "                alpha_e_flag = parts_setopt_lens[8].replace(')', '')\n",
    "\n",
    "                line = find_last_line('lens   ein', content_opt)\n",
    "                line_content = content_opt.split('\\n')[line - 1]\n",
    "                line_list = line_content.split()\n",
    "\n",
    "                chi = find_last_line('chi^2', content_opt)\n",
    "                chi_content = content_opt.split('\\n')[chi - 1]\n",
    "                chi_list = chi_content.split()\n",
    "\n",
    "                row_0 = [name, 'M', 'x', 'y', 'e', '$θ_{e}$', 'c or $r_{s}$', '$α_{e}$']\n",
    "                row_1 = ['Input Value', m, x, y, e, pa, ce_rs, alpha_e]\n",
    "                row_2 = ['Opt Result', line_list[3], line_list[4], line_list[5], line_list[6], line_list[7], line_list[8], line_list[9]]\n",
    "                row_3 = ['Fixed', m_flag, x_flag, y_flag, e_flag, pa_flag, ce_rs_flag, alpha_e_flag]\n",
    "\n",
    "                table = pd.DataFrame([row_1, row_2, row_3], columns = row_0)\n",
    "            \n",
    "            if show_params:\n",
    "                print(table)\n",
    "\n",
    "            if save_table_flag:\n",
    "                table.to_csv('table.csv')\n",
    "            \n",
    "            return table, chi_list\n",
    "\n",
    "    print(\"Model not found\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "# Position and Magnification Plots\n",
    "\n",
    "def error_plot(filename_1, filename_2, filename_4, filename_5, plot_name, num_images, title, \n",
    "               table_flag = False, glafic_file_1=None, glafic_file_2=None, shear = False, show_shear = False, show_params = False):\n",
    "    \n",
    "    if table_flag:\n",
    "        if glafic_file_1 is None:\n",
    "            print(\"Please provide the filename for the glafic script\")\n",
    "            raise ValueError(\"Glafic File not provided\")\n",
    "        elif glafic_file_2 is None:\n",
    "            print(\"Please provide the Opt Result filename for the glafic script\")\n",
    "            raise ValueError(\"Glafic File not provided\")\n",
    "\n",
    "    if table_flag:    \n",
    "        table, chi2 = glafic_tabular(glafic_file_1, glafic_file_2, show_params=show_params)\n",
    "        if table is None:\n",
    "            raise ValueError(\"Failed to create the table from the glafic file.\")\n",
    "\n",
    "    if shear:\n",
    "        table_shear = glafic_tabular(glafic_file_1, glafic_file_2, shear = True, show_shear = show_shear, show_params=show_params)\n",
    "        if table_shear is None:\n",
    "            raise ValueError(\"Failed to create the table from the glafic file.\")\n",
    "        \n",
    "    # Storage for parsed data\n",
    "    data = []\n",
    "    \n",
    "    # val = pd.read_csv(filename_4)\n",
    "    # val.__dataframe__\n",
    "    # val_column = val.columns[0]\n",
    "\n",
    "    # # Split the values in the data_column and expand into separate columns\n",
    "    # val = val[val_column].str.split(expand=True)\n",
    "\n",
    "    # # Convert the DataFrame to numeric type\n",
    "    # val = val.apply(pd.to_numeric)\n",
    "\n",
    "    # print(val)\n",
    "\n",
    "    with open(filename_2, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip lines starting with \"#\"\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            \n",
    "            # Split the line by whitespace\n",
    "            line_data = line.split()\n",
    "            \n",
    "            # Remove # \n",
    "            line_data = [float(val) for val in line_data if val != '#']  \n",
    "            \n",
    "            data.append(line_data)\n",
    "    \n",
    "    val = pd.DataFrame(data)\n",
    "    val = val.iloc[1:]\n",
    "    brightest_flux = max(val[2])\n",
    "    brightest_index = val[2].idxmax()\n",
    "\n",
    "    # Add a column called 'colour' to val\n",
    "    val['colour'] = ''\n",
    "\n",
    "    # Assign 'red' to the brightest image\n",
    "    val.at[brightest_index, 'colour'] = 'red'\n",
    "\n",
    "    # Get the coordinates of the brightest image\n",
    "    brightest_x = val.at[brightest_index, 0]\n",
    "    brightest_y = val.at[brightest_index, 1]\n",
    "\n",
    "    # Calculate the angles of the other images relative to the brightest image\n",
    "    angles = np.arctan2(val[1] - brightest_y, val[0] - brightest_x)\n",
    "\n",
    "    # Sort the indices of the images by angle in clockwise order\n",
    "    sorted_indices = angles.sort_values(ascending=False).index\n",
    "\n",
    "    # Assign colours to the remaining images\n",
    "    colours = ['green', 'yellow', 'blue']\n",
    "    colour_index = 0\n",
    "    for idx in sorted_indices:\n",
    "        if val.at[idx, 'colour'] == '':\n",
    "            val.at[idx, 'colour'] = colours[colour_index]\n",
    "            colour_index += 1\n",
    "    \n",
    "    val['flux_ratio'] = abs(val[2] / brightest_flux)\n",
    "\n",
    "    err_1 = Complex(val[2][1], val[4][1])\n",
    "    err_2 = Complex(val[2][2], val[4][2])\n",
    "    err_3 = Complex(val[2][3], val[4][3])\n",
    "    err_4 = Complex(val[2][4], val[4][4])\n",
    "\n",
    "    ratio_1 = str(Complex.truediv(err_1, err_2))\n",
    "    ratio_2 = str(Complex.truediv(err_2, err_2))\n",
    "    ratio_3 = str(Complex.truediv(err_3, err_2))\n",
    "    ratio_4 = str(Complex.truediv(err_4, err_2))\n",
    "\n",
    "    ratio_1_error = float(ratio_1.split(' ± ')[1])\n",
    "    ratio_2_error = float(ratio_2.split(' ± ')[1])\n",
    "    ratio_3_error = float(ratio_3.split(' ± ')[1])\n",
    "    ratio_4_error = float(ratio_4.split(' ± ')[1])\n",
    "\n",
    "    val['flux_ratio_error'] = [ratio_1_error, ratio_2_error, ratio_3_error, ratio_4_error]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Line by line read (Remove # from obs file)\n",
    "    with open(filename_1, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip lines starting with \"#\"\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            \n",
    "            # Split the line by whitespace\n",
    "            line_data = line.split()\n",
    "            \n",
    "            # Remove # \n",
    "            line_data = [float(val) for val in line_data if val != '#']  \n",
    "            \n",
    "            data.append(line_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "    # Exclude the first row\n",
    "    data_df = data_df.iloc[1:]\n",
    "    \n",
    "    data_df.insert(8, \"Label\", val['colour'].map({'red': 'Red Image', 'green': 'Green Image', 'yellow': 'Yellow Image', 'blue': 'Blue Image'}), True)\n",
    "\n",
    "    # Reorder the images according to the order red, green, yellow, blue\n",
    "    order = ['Red Image', 'Green Image', 'Yellow Image', 'Blue Image']\n",
    "    data_df['Label'] = pd.Categorical(data_df['Label'], categories=order, ordered=True)\n",
    "    data_df = data_df.sort_values('Label').reset_index(drop=True)\n",
    "\n",
    "    data_df = data_df.drop(columns =[3, 5, 6, 7])\n",
    "\n",
    "    # Read and process the predicted data\n",
    "    data_pred = pd.read_csv(filename_4, header=None, delim_whitespace=True, comment='#')\n",
    "    df_pred = data_pred.iloc[1:]\n",
    "\n",
    "    obs_point = data_df[[0, 1]].reset_index(drop=True)\n",
    "    pred_point = df_pred[[0, 1]].reset_index(drop=True)\n",
    "\n",
    "    # Row Switching\n",
    "    dist_matrix = cdist(obs_point.to_numpy(), pred_point.to_numpy(), 'euclidean')\n",
    "\n",
    "    obs_indices, pred_indices_optimal = linear_sum_assignment(dist_matrix)\n",
    "\n",
    "    pred_point_sorted = pred_point.iloc[pred_indices_optimal].reset_index(drop=True)\n",
    "\n",
    "    df_pred_sorted_full = df_pred.iloc[pred_indices_optimal].reset_index(drop=True)\n",
    "\n",
    "    df_pred = df_pred_sorted_full\n",
    "    df_pred = df_pred.drop(columns =[3])\n",
    "\n",
    "\n",
    "    # Eliminating the 5th image\n",
    "    if len(df_pred[2])>num_images:\n",
    "        i = len(df_pred[2]) - num_images\n",
    "        for j in range(i):\n",
    "            min_vales = np.min(abs(df_pred[2]))\n",
    "            df_2 = abs(df_pred)\n",
    "            b = df_2.index.get_loc(df_2[df_2[2] == min_vales].index[0])\n",
    "            df_3 = df_pred.drop((b+1), axis='index')\n",
    "            df_pred = df_3\n",
    "            df_pred.reset_index(drop=True, inplace=True)\n",
    "        df_pred.index = df_pred.index + 1\n",
    "\n",
    "    df_pred = df_pred.reset_index(drop=True)\n",
    "\n",
    "    # Calculations for Position Error values\n",
    "    d_x = (abs(data_df[0]) - abs(df_pred[0])).abs()\n",
    "    d_y = (abs(data_df[1]) - abs(df_pred[1])).abs()\n",
    "    sum_sq = (d_x**2) + (d_y**2)\n",
    "    sq = np.sqrt(sum_sq)\n",
    "    rms = np.average(sq)\n",
    "    rms_unit = rms*1000\n",
    "    rms_round = round(rms_unit, 3)/1000\n",
    "    rms_str = str(rms_round)\n",
    "\n",
    "    print(sq)\n",
    "\n",
    "    # Plotting Position Error Graph\n",
    "    # custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "    # sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "\n",
    "    colours1 = ['lightsalmon', 'green', 'gold', 'blue']\n",
    "    plt.figure(figsize=(15, 5), dpi=500)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(data_df['Label'], sq, color = colours1, width=0.3)\n",
    "    plt.axhline(xmin=0.045, xmax=0.13, y=1.28/1000, linestyle ='--', color ='r', linewidth = 2, label='1 σ Error')\n",
    "    plt.axhline(xmin=0.32, xmax=0.40, y=0.85/1000, linestyle ='--', color ='r', linewidth = 2)\n",
    "    plt.axhline(xmin=0.6, xmax=0.68, y=0.83/1000, linestyle ='--', color ='r', linewidth = 2)\n",
    "    plt.axhline(xmin=0.87, xmax=0.95, y=0.39/1000, linestyle ='--', color ='r', linewidth = 2)\n",
    "    plt.title('Position Diagnostic (ΔRMS = ' + rms_str + ')', fontsize=10)\n",
    "    plt.legend(loc='upper right', fontsize='small')\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.ylim(0, max(sq)+0.005)\n",
    "    plt.xlabel('Images')\n",
    "    plt.ylabel('Positional offset')\n",
    "\n",
    "    # Calculations for Magnification value\n",
    "    # Previous Code: f = df_pred[2][1]\n",
    "    max_flux = max(abs(df_pred[2]))\n",
    "    flux_ratio = df_pred[2]/max_flux\n",
    "    df_pred[3] = abs(flux_ratio)\n",
    "\n",
    "    # Create data \n",
    "    x = np.arange(4)\n",
    "    width = 0.3\n",
    "\n",
    "    # FITS image processing for predicted flux at observed positions \n",
    "    image = fits.open(filename_5)\n",
    "    values = 1/image[0].data\n",
    "    image.close()\n",
    "    dat = values[6]\n",
    "    g = (data_df[0]/0.01).astype(int) - 2000\n",
    "    h = (data_df[1]/0.01).astype(int) - 2000\n",
    "\n",
    "    g_max = (g + 1).astype(int)\n",
    "    h_max = (h + 1).astype(int)\n",
    "    g_min = (g - 1).astype(int)\n",
    "    h_min = (h - 1).astype(int)\n",
    "\n",
    "\n",
    "    flux_pos = []\n",
    "    flux_pos_max = []\n",
    "    flux_pos_min = []\n",
    "\n",
    "    for i in range(0,4):\n",
    "        flux_cal = dat[h[i]][g[i]]\n",
    "        flux_pos.append(abs(flux_cal))\n",
    "    \n",
    "\n",
    "    for i in range(0,4):\n",
    "        flux_cal_max = dat[h_max[i]][g_max[i]]\n",
    "        flux_pos_max.append(abs(flux_cal_max))\n",
    "\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        flux_cal_min = dat[h_min[i]][g_min[i]]\n",
    "        flux_pos_min.append(abs(flux_cal_min))\n",
    "    \n",
    "    arrow_legnths = (np.array(flux_pos_max) - np.array(flux_pos_min))/100\n",
    "    # arrow_legnths = [0,0,0,0]\n",
    "    true_flux = np.array(flux_pos)\n",
    "\n",
    "    # Switch columns of val according to the previous colour indexing\n",
    "    val = val.set_index('colour').loc[['red', 'green', 'yellow', 'blue']].reset_index()\n",
    "\n",
    "    height = max(val[2]) + 5\n",
    "    # Plotting Flux Error Graph\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(x+0.15, val[2], width, color='red', edgecolor ='k', label = 'μ_obs/μ_ref') \n",
    "    plt.bar(x-0.15, abs(df_pred[2]), width, color='white', edgecolor='k', hatch='\\\\/', label='μ_pred/μ_ref') \n",
    "    plt.bar(x+0.45, true_flux, width, color='orange', edgecolor='k', label = 'µ_pred/μ_ref (obs pos)')\n",
    "    # plt.errorbar(x-0.15, val[0], yerr=3*(val[1]), fmt='o', color='black', capsize=4, label='3 σ Error') (OLD ERROR BAR)\n",
    "    plt.errorbar(x+0.15, val[2], yerr=val[4], fmt='.', color='black', capsize=5, label='1 σ Error')\n",
    "    plt.xticks(x+0.15, data_df['Label'], fontsize=8) \n",
    "    plt.arrow(x[0]+0.45, true_flux[0], 0, arrow_legnths[0], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[0]+0.45, true_flux[0], 0, -arrow_legnths[0], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[1]+0.45, true_flux[1], 0, arrow_legnths[1], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[1]+0.45, true_flux[1], 0, -arrow_legnths[1], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[2]+0.45, true_flux[2], 0, arrow_legnths[2], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[2]+0.45, true_flux[2], 0, -arrow_legnths[2], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[3]+0.45, true_flux[3], 0, arrow_legnths[3], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.arrow(x[3]+0.45, true_flux[3], 0, -arrow_legnths[3], head_width=0.1, head_length=0.4, fc='k', ec='k')\n",
    "    plt.xlabel(\"Images\") \n",
    "    plt.ylabel(\"Magnifications\") \n",
    "    plt.ylim()\n",
    "    plt.legend(loc = 'upper right', fontsize='small')\n",
    "    plt.title('Magnification Diagnostic', fontsize=10)\n",
    "    plt.suptitle(title)\n",
    "    if table_flag:\n",
    "        table_plot = plt.table(cellText=table.values, colLabels=table.columns, cellLoc = 'center', loc='bottom', bbox=[-1.0, -0.5, 3.0, 0.3])\n",
    "        table_plot.auto_set_font_size(False)\n",
    "        table_plot.set_fontsize(10)\n",
    "        plt.suptitle('Lens ' + plot_name + ' constrained' + ' (Chi2 = ' + chi2[2] + ')')\n",
    "        if shear == True:\n",
    "            table_shear_plot = plt.table(cellText=table_shear.values, colLabels=table_shear.columns, cellLoc = 'center', loc='bottom', bbox=[-1.0, -0.9, 3.0, 0.3])\n",
    "            table_shear_plot.auto_set_font_size(False)\n",
    "            table_shear_plot.set_fontsize(10)\n",
    "\n",
    "    return data_df, df_pred\n",
    "\n",
    "\n",
    "\n",
    "# Critical Curves Plot\n",
    "\n",
    "def critcurve_plot(filename_4, filename_3, pos_output, num_images):\n",
    "    data_crit = pd.read_csv(filename_3, header= None, sep=\"\\s+\")\n",
    "    data_crit.__dataframe__\n",
    "    df = data_crit.iloc[1:]\n",
    "\n",
    "    obs = pd.DataFrame(pos_output)\n",
    "\n",
    "    # Initialize empty list \n",
    "    data = []\n",
    "\n",
    "    # Read and process the predicted data\n",
    "    de = pd.read_csv(filename_4, header=None, delim_whitespace=True, comment='#')\n",
    "    de = de.iloc[1:]\n",
    "\n",
    "    # Function for swapping data \n",
    "    def swap_rows(df, row1, row2):\n",
    "        df.iloc[row1], df.iloc[row2] =  df.iloc[row2].copy(), df.iloc[row1].copy()\n",
    "        return df\n",
    "    \n",
    "    # For loop to iterate over row range for row swapping\n",
    "    for i in range(4):\n",
    "        diff = abs(abs(obs.iloc[i,0]) - abs(de[0]))\n",
    "        m = diff.idxmin()\n",
    "        n = min(diff)\n",
    "        if n < 0.1:\n",
    "            de = swap_rows(de, i, (m-1))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        \n",
    "    de = de.drop(columns =[3])\n",
    "\n",
    "    # Eliminating the 5th image\n",
    "    if len(de[2])>num_images:\n",
    "        i = len(de[2]) - num_images\n",
    "        for j in range(i):\n",
    "            min_vales = np.min(abs(de[2]))\n",
    "            df_4 = abs(de)\n",
    "            b = df_4.index.get_loc(df_4[df_4[2] == min_vales].index[0])\n",
    "            df_5 = de.drop((b+1), axis='index')\n",
    "            de = df_5\n",
    "            de.reset_index(drop=True, inplace=True)\n",
    "        de.index = de.index + 1\n",
    "\n",
    "    labels = ['A', 'B', 'C', 'D']\n",
    "\n",
    "    # Plotting Critial Curves\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(df[0]/0.04, df[1]/0.04, s=2, color = 'orange', label = 'Critical Curve')\n",
    "    plt.scatter(df[2]/0.04, df[3]/0.04, s=2)\n",
    "    plt.scatter(df[4]/0.04, df[5]/0.04, s=2)\n",
    "    plt.scatter(df[6]/0.04, df[7]/0.04, s=2, label = 'Caustics')\n",
    "\n",
    "    height_1 = max(df[0]/0.04)\n",
    "    height_2 = max(df[1]/0.04)\n",
    "\n",
    "    # colors = ['red',  'blue', 'green', 'gold']\n",
    "    colors = obs['Label'].map({'Red Image': 'red', 'Green Image': 'green', 'Yellow Image': 'gold', 'Blue Image': 'blue'}).tolist()\n",
    "\n",
    "    # Plotting obs image positions and labels \n",
    "    plt.scatter(de[0]/0.04, de[1]/0.04, s = 100, marker= '+', color = colors, alpha = 0.5, label = 'Predicted Position')\n",
    "    plt.scatter(obs[0]/0.04, obs[1]/0.04, s=15, color = colors, marker = 'o', label = 'Observed Position')\n",
    "    # for x, y, txt in zip(data_df[0]*100, data_df[1]*100, labels):\n",
    "    #     plt.text(x, y-17, txt, fontsize=13, ha='center', va='bottom')\n",
    "\n",
    "    plt.title('Critical Curves and Caustics', fontsize=10)\n",
    "    plt.legend(loc='upper right', fontsize='small')\n",
    "    plt.tick_params(labelsize=8)\n",
    "    plt.xlabel('x [Pixel]')\n",
    "    plt.ylabel('y [Pixel]', labelpad=0)\n",
    "    # Calculate the natural limits of the graph\n",
    "    x_min, x_max = min(df[0]/0.04), max(df[0]/0.04)\n",
    "    y_min, y_max = min(df[1]/0.04), max(df[1]/0.04)\n",
    "\n",
    "    # Define a zoom factor\n",
    "    zoom_factor = 0.2\n",
    "\n",
    "    # Calculate the limits with the zoom factor\n",
    "    x_range = (x_max - x_min) * zoom_factor\n",
    "    y_range = (y_max - y_min) * zoom_factor\n",
    "\n",
    "    plt.xlim(x_min - x_range, x_max + x_range)\n",
    "    plt.ylim(y_min - y_range, y_max + y_range)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to make both tables\n",
    "def make_tables(model_ver, model_path, constraint):\n",
    "    # Load the data\n",
    "    with open(model_path + '/' + model_ver + '_optresult' + '.dat', 'r') as file:\n",
    "        opt_result = file.readlines()\n",
    "\n",
    "    # Find the last line with 'optimize' in it\n",
    "    last_optimize_index = None\n",
    "    for idx in range(len(opt_result) - 1, -1, -1):\n",
    "        if 'optimize' in opt_result[idx]:\n",
    "            last_optimize_index = idx\n",
    "            last_optimize_line = opt_result[idx]\n",
    "            break\n",
    "    if last_optimize_index is None:\n",
    "        raise ValueError(\"No line with 'optimize' found in the file.\")\n",
    "\n",
    "    # Extract everything after the last 'optimize' line\n",
    "    opt_result = opt_result[last_optimize_index + 1:]\n",
    "\n",
    "    # Count the number of lines that start with 'lens'\n",
    "    lens_count = sum(1 for line in opt_result if line.startswith('lens'))\n",
    "\n",
    "    # Initialize a dictionary to hold the lens parameters\n",
    "    lens_params_dict = {}\n",
    "\n",
    "    # Extract the lens parameters\n",
    "    lens_params = []\n",
    "    for line in opt_result:\n",
    "        if line.startswith('lens'):\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            lens_name = parts[1]\n",
    "            params = [float(x) for x in parts[2:]]\n",
    "\n",
    "            # Store the parameters in the dictionary\n",
    "            lens_params_dict[lens_name] = params\n",
    "            lens_params.append((lens_name, params))\n",
    "\n",
    "    # Remove the first lens parameter\n",
    "    if lens_params:\n",
    "        for i in range(len(lens_params)):\n",
    "            lens_name, params = lens_params[i]\n",
    "            lens_params_dict[lens_name] = params[1:]\n",
    "\n",
    "    # Number of len profiles\n",
    "    num_lens_profiles = len(lens_params_dict)\n",
    "\n",
    "    # Use generic column names: param1, param2, ...\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    max_param_len = 0\n",
    "\n",
    "    for lens_name, params in lens_params_dict.items():\n",
    "        row = {'Lens Name': lens_name}\n",
    "        for i, val in enumerate(params):\n",
    "            row[f'param{i+1}'] = val\n",
    "        rows.append(row)\n",
    "        if len(params) > max_param_len:\n",
    "            max_param_len = len(params)\n",
    "\n",
    "    columns = ['Lens Name'] + [f'param{i+1}' for i in range(max_param_len)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    # Load the input parameters from the Python file\n",
    "    with open(model_path + '/' + constraint + '_point' + '.py', 'r') as file:\n",
    "        py = file.readlines()\n",
    "\n",
    "    # Extracting the input parameters from the Python file\n",
    "    set_lens_lines = [line for line in py if line.startswith('glafic.set_lens(')]\n",
    "    if not set_lens_lines:\n",
    "        raise ValueError(\"No lines starting with 'glafic.set_lens(' found in the file.\")\n",
    "\n",
    "    set_lens_params = []\n",
    "    for line in set_lens_lines:\n",
    "        match = re.search(r'set_lens\\((.*?)\\)', line)\n",
    "        if match:\n",
    "            params_str = match.group(1)\n",
    "            params = [param.strip() for param in params_str.split(',')]\n",
    "            set_lens_params.append(params)\n",
    "        else:\n",
    "            raise ValueError(f\"No valid parameters found in line: {line.strip()}\")\n",
    "\n",
    "    # Store the parameters in a dictionary\n",
    "    set_lens_dict = {}\n",
    "    for params in set_lens_params:\n",
    "        if len(params) < 3:\n",
    "            raise ValueError(f\"Not enough parameters found in line: {params}\")\n",
    "        lens_name = params[1].strip(\"'\\\"\")  # Remove quotes from lens name\n",
    "        lens_params = [float(x) for x in params[2:]]  # Skip index and lens name\n",
    "        set_lens_dict[lens_name] = lens_params\n",
    "\n",
    "    # Remove the first lens parameter\n",
    "    if set_lens_dict:\n",
    "        for lens_name, params in set_lens_dict.items():\n",
    "            set_lens_dict[lens_name] = params[1:]  # Remove the first parameter (index)\n",
    "\n",
    "    # Use generic column names: param1, param2, ...\n",
    "    df_input = pd.DataFrame()\n",
    "    rows_input = []\n",
    "    max_param_len_input = 0\n",
    "    for lens_name, params in set_lens_dict.items():\n",
    "        row = {'Lens Name': lens_name}\n",
    "        for i, val in enumerate(params):\n",
    "            row[f'param{i+1}'] = val\n",
    "        rows_input.append(row)\n",
    "        if len(params) > max_param_len_input:\n",
    "            max_param_len_input = len(params)\n",
    "    columns_input = ['Lens Name'] + [f'param{i+1}' for i in range(max_param_len_input)]\n",
    "    df_input = pd.DataFrame(rows_input, columns=columns_input)\n",
    "    \n",
    "    # Extract input flags from the Python file\n",
    "    set_flag_lines = [line for line in py if line.startswith('glafic.setopt_lens(')]\n",
    "    if not set_flag_lines:\n",
    "        raise ValueError(\"No lines starting with 'glafic.setopt_lens(' found in the file.\")\n",
    "    set_flag_params = []\n",
    "    for line in set_flag_lines:\n",
    "        match = re.search(r'setopt_lens\\((.*?)\\)', line)\n",
    "        if match:\n",
    "            params_str = match.group(1)\n",
    "            params = [param.strip() for param in params_str.split(',')]\n",
    "            set_flag_params.append(params)\n",
    "        else:\n",
    "            raise ValueError(f\"No valid parameters found in line: {line.strip()}\")\n",
    "    \n",
    "    # Store the parameters in a dictionary\n",
    "    set_flag_dict = {}\n",
    "    for params in set_flag_params:\n",
    "        if len(params) < 2:\n",
    "            raise ValueError(f\"Not enough parameters found in line: {params}\")\n",
    "        # The lens name is not present in setopt_lens, so use the lens index to map to set_lens_dict\n",
    "        lens_index = params[0].strip(\"'\\\"\")\n",
    "        # Find the lens name corresponding to this index from set_lens_params\n",
    "        lens_name = None\n",
    "        for lens_params in set_lens_params:\n",
    "            if lens_params[0].strip(\"'\\\"\") == lens_index:\n",
    "                lens_name = lens_params[1].strip(\"'\\\"\")\n",
    "                break\n",
    "        if lens_name is None:\n",
    "            raise ValueError(f\"Lens name for index {lens_index} not found in set_lens_params\")\n",
    "        flag = ','.join(params[1:])  # Join all flag values as a string\n",
    "        set_flag_dict[lens_name] = flag\n",
    "   \n",
    "    # Remove the first flag parameter\n",
    "    if set_flag_dict:\n",
    "        for lens_name, flag in set_flag_dict.items():\n",
    "            flag_parts = flag.split(',')\n",
    "            set_flag_dict[lens_name] = ','.join(flag_parts[1:])  # Remove the first flag parameter\n",
    "    \n",
    "    # Dynamically create columns: 'Lens Name', 'flag1', 'flag2', ..., based on the maximum number of flags\n",
    "    df_flag = pd.DataFrame()\n",
    "    rows_flag = []\n",
    "    max_flag_len = 0\n",
    "    \n",
    "    # First, determine the maximum number of flags\n",
    "    for flag in set_flag_dict.values():\n",
    "        flag_parts = flag.split(',')\n",
    "        if len(flag_parts) > max_flag_len:\n",
    "            max_flag_len = len(flag_parts)\n",
    "    for lens_name, flag in set_flag_dict.items():\n",
    "        flag_parts = flag.split(',')\n",
    "        row = {'Lens Name': lens_name}\n",
    "        for i, val in enumerate(flag_parts):\n",
    "            row[f'flag{i+1}'] = val\n",
    "        rows_flag.append(row)\n",
    "    columns_flag = ['Lens Name'] + [f'flag{i+1}' for i in range(max_flag_len)]  \n",
    "    df_flag = pd.DataFrame(rows_flag, columns=columns_flag)\n",
    "    \n",
    "    # Combine all dataframes into a list of dataframes for each lens\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(num_lens_profiles):\n",
    "        lens_name = df['Lens Name'][i]\n",
    "        \n",
    "        # Find the model type (case-insensitive match)\n",
    "        model_type = None\n",
    "        for m in model_list:\n",
    "            if m.lower() == lens_name.lower():\n",
    "                model_type = m\n",
    "                break\n",
    "        if model_type is None:\n",
    "            continue\n",
    "\n",
    "        symbols = model_params[model_type][:7]\n",
    "        # Row 2: input\n",
    "        row_input = pd.DataFrame([df_input.iloc[i, 1:8].values], columns=symbols)\n",
    "        # Row 3: output\n",
    "        row_output = pd.DataFrame([df.iloc[i, 1:8].values], columns=symbols)\n",
    "        # Row 4: flags\n",
    "        row_flags = pd.DataFrame([df_flag.iloc[i, 1:8].values], columns=symbols)\n",
    "\n",
    "        # Stack vertically, add a label column for row type\n",
    "        lens_df = pd.concat([\n",
    "            row_input.assign(Type='Input'),\n",
    "            row_output.assign(Type='Output'),\n",
    "            row_flags.assign(Type='Flag')\n",
    "        ], ignore_index=True)\n",
    "        lens_df.insert(0, 'Lens Name', lens_name)\n",
    "        \n",
    "        # Move 'Type' to the second column\n",
    "        cols = lens_df.columns.tolist()\n",
    "        cols.insert(1, cols.pop(cols.index('Type')))\n",
    "        lens_df = lens_df[cols]\n",
    "        dfs.append(lens_df)\n",
    "    \n",
    "    # Anomaly Calculation\n",
    "    columnn_names = ['x', 'y', 'mag', 'pos_err', 'mag_err', '1', '2', '3']\n",
    "    obs_point = pd.read_csv('obs_point/obs_point_(POS+FLUX).dat', delim_whitespace=True, header=None, skiprows=1, names=columnn_names)\n",
    "    out_point = pd.read_csv(model_path + '/' + model_ver + '_point.dat', delim_whitespace=True, header=None, skiprows=1, names=columnn_names)\n",
    "    out_point.drop(columns=['mag_err', '1', '2', '3'], inplace=True)\n",
    "\n",
    "    # Drop rows in obs_point where the corresponding out_point['mag'] < 1\n",
    "    mask = abs(out_point['mag']) >= 1\n",
    "    out_point = out_point[mask[:len(out_point)]].reset_index(drop=True)\n",
    "    out_point['x_diff'] = abs(out_point['x'] - obs_point['x'])\n",
    "    out_point['y_diff'] = abs(out_point['y'] - obs_point['y'])\n",
    "    out_point['mag_diff'] = abs(abs(out_point['mag']) - abs(obs_point['mag']))\n",
    "    out_point['pos_sq'] = np.sqrt(out_point['x_diff']**2 + out_point['y_diff']**2)  # Plotted on graph\n",
    "\n",
    "    # RMS\n",
    "    pos_rms = np.average(out_point['pos_sq'])\n",
    "    print(f\"RMS of the differences: {pos_rms:.6f}\")\n",
    "\n",
    "    mag_rms = np.average(np.sqrt(out_point['mag_diff']**2))\n",
    "    print(f\"RMS of the magnification differences: {mag_rms:.6f}\")\n",
    "\n",
    "    # Position Anomalies\n",
    "    one_sigma_pos = obs_point['pos_err'] * 1.0\n",
    "    out_point['pos_anomaly'] = np.where(out_point['pos_sq'] > one_sigma_pos, 'Anomaly', 'Normal')\n",
    "    \n",
    "    # Magnification Anomalies\n",
    "    one_sigma_mag = obs_point['mag_err'] * 1.0\n",
    "    one_sigma_mag_up = obs_point['mag'] + one_sigma_mag\n",
    "    one_sigma_mag_down = obs_point['mag'] - one_sigma_mag\n",
    "    out_point['mag_anomaly'] = np.where(\n",
    "        (abs(out_point['mag']) > one_sigma_mag_up) | (abs(out_point['mag']) < one_sigma_mag_down),\n",
    "        'Anomaly',\n",
    "        'Normal'\n",
    "    )\n",
    "\n",
    "    # Combine the anomalies into a single column\n",
    "    out_point['index'] = np.arange(len(out_point)) + 1\n",
    "    out_point['pos_anom'] = abs(out_point['pos_sq'] - one_sigma_pos)\n",
    "\n",
    "    out_point['mag_anom'] = np.minimum(\n",
    "        abs(out_point['mag_diff'] - one_sigma_mag_up),\n",
    "        abs(out_point['mag_diff'] - one_sigma_mag_down)\n",
    "    )\n",
    "\n",
    "    # Replace anomaly flags with ticks/crosses\n",
    "    def flag_to_symbol(flag):\n",
    "        return '$\\checkmark$' if flag == 'Anomaly' else 'x'\n",
    "    \n",
    "    out_point['pos_sq'] = out_point['pos_sq'].round(8)\n",
    "    out_point['mag_diff'] = out_point['mag_diff'].round(4)\n",
    "    out_point['pos_anom'] = out_point['pos_anom'].round(6)\n",
    "    out_point['mag_anom'] = out_point['mag_anom'].round(4)\n",
    "    \n",
    "    # Plot the two tables together\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), gridspec_kw={'height_ratios': [1, 1]})\n",
    "    \n",
    "    # First table: Lens Parameters\n",
    "    ax1.axis('off')\n",
    "    y_offset = 0.95  # Start at top\n",
    "    row_height = 0.1  # adjust to control row spacing visually\n",
    "    spacing = 0.1     # space between tables\n",
    "\n",
    "    for i, lens_df in enumerate(dfs):\n",
    "        nrows = lens_df.shape[0]\n",
    "        table_height = row_height * (nrows + 1)\n",
    "        y_pos = y_offset - i * (table_height + spacing)\n",
    "\n",
    "        table_data = lens_df.iloc[:, 1:].values\n",
    "        col_labels_lens = lens_df.columns[1:]\n",
    "\n",
    "        table = ax1.table(\n",
    "            cellText=table_data,\n",
    "            colLabels=col_labels_lens,\n",
    "            cellLoc='center',\n",
    "            loc='center',\n",
    "            bbox=[0, y_pos - table_height, 1, table_height]\n",
    "        )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(11)\n",
    "\n",
    "        lens_label = f\"   Lens: {lens_df['Lens Name'][0].upper()}   \"\n",
    "        ax1.text(0.5, y_pos + 0.01, lens_label, ha='center', va='bottom',\n",
    "                 fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f\"Lens Parameters and Anomalies for {model_ver}\", fontsize=14, y=0.95)\n",
    "    \n",
    "    # Second table: Anomalies\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    table_data = out_point[['index', 'pos_sq', 'mag_diff', 'pos_anom', 'mag_anom', 'pos_anomaly', 'mag_anomaly']].copy()\n",
    "    table_data['pos_anomaly'] = table_data['pos_anomaly'].apply(flag_to_symbol)\n",
    "    table_data['mag_anomaly'] = table_data['mag_anomaly'].apply(flag_to_symbol)\n",
    "    rms_row = [\n",
    "    '',  # index column\n",
    "    f'RMS: {pos_rms:.6f}',\n",
    "    f'RMS: {mag_rms:.4f}',\n",
    "    '',  # pos_anomaly column\n",
    "    '',   # mag_anomaly column\n",
    "    '',\n",
    "    ''\n",
    "    ]\n",
    "\n",
    "    table_values = table_data.values.tolist() + [rms_row]\n",
    "    col_labels = ['Image', 'Pos Sum of Diff', 'Mag Diff', 'Pos Diff (1 $\\sigma$)', 'Mag Diff (1 $\\sigma$)', 'Pos Anomaly', 'Mag Anomaly']\n",
    "    \n",
    "    # Define column labels for the anomaly table\n",
    "    table = ax2.table(\n",
    "        cellText=table_values,\n",
    "        colLabels=col_labels,\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        bbox=[0, 0, 1, 1]\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 1.2)\n",
    "    plt.title(f\"Anomalies in {model_ver} Model\", fontsize=12, pad=10)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit title\n",
    "    plt.show()\n",
    "    return dfs, out_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lens corresponding parameters (order preserved)\n",
    "# POW\n",
    "pow_params = ['$z_{s,fid}$', 'x', 'y', 'e', '$θ_{e}$', '$r_{Ein}$', '$\\gamma$ (PWI)']\n",
    "\n",
    "# SIE\n",
    "sie_params = ['$\\sigma$', 'x', 'y', 'e', '$θ_{e}$', '$r_{core}$', 'NaN']\n",
    "\n",
    "# NFW\n",
    "nfw_params = ['M', 'x', 'y', 'e', '$θ_{e}$', 'c or $r_{s}$', 'NaN']\n",
    "\n",
    "# EIN\n",
    "ein_params = ['M', 'x', 'y', 'e', '$θ_{e}$', 'c or $r_{s}$', r'$\\alpha_{e}$']\n",
    "\n",
    "# SHEAR \n",
    "shear_params = ['$z_{s,fid}$', 'x', 'y', '$\\gamma$', '$θ_{\\gamma}$', 'NaN', '$\\kappa$']\n",
    "\n",
    "# Sersic\n",
    "sersic_params = ['$M_{tot}$', 'x', 'y', 'e', '$θ_{e}$', '$r_{e}$', '$n$']\n",
    "\n",
    "# Cored SIE\n",
    "cored_sie_params = ['M', 'x', 'y', 'e', '$θ_{e}$', '$r_{core}$', 'NaN']\n",
    "\n",
    "model_list = ['POW', 'SIE', 'ANFW', 'EIN', 'PERT', 'SERS']\n",
    "model_params = {\n",
    "    'POW': pow_params,\n",
    "    'SIE': sie_params,\n",
    "    'ANFW': nfw_params,\n",
    "    'EIN': ein_params,\n",
    "    'PERT': shear_params,\n",
    "    'SERS': sersic_params\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facd71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name_1 = 'SIE_POS'\n",
    "loc = 'Test/Shear Shift/'\n",
    "title = 'SIE + SHEAR Model (Position Constrained)'\n",
    "\n",
    "if 'POS+FLUX' in plot_name_1 or 'pos+flux' in plot_name_1:\n",
    "    constraint = 'pos_flux'\n",
    "else:\n",
    "    constraint = 'pos'\n",
    "\n",
    "filename_0 = loc + constraint + '_point.py'\n",
    "filename_1 = 'obs_point/obs_point_(POS).dat'\n",
    "filename_2 = 'obs_point/obs_point_(POS+FLUX).dat'\n",
    "filename_3 = loc + plot_name_1 + '_crit.dat'\n",
    "filename_4 = loc + plot_name_1 + '_point.dat'\n",
    "filename_5 = loc + plot_name_1 + '_lens.fits'\n",
    "filename_6 = loc + plot_name_1 + '_optresult.dat'\n",
    "num_images = 4\n",
    "plot_name = plot_name_1\n",
    "\n",
    "if 'SHEAR' in plot_name or 'shear' in plot_name:\n",
    "    shear_set = True\n",
    "else:\n",
    "    shear_set = False\n",
    "\n",
    "if 'FLUX' in plot_name or 'flux' in plot_name or 'Flux' in plot_name:\n",
    "    filename_1 = 'obs_point/obs_point_(POS+FLUX).dat'\n",
    "\n",
    "glafic_tabular(filename_0, filename_6, save_table_flag = False, shear=shear_set, show_shear=False, show_params=False)\n",
    "\n",
    "obs_data, pred_data = error_plot(filename_1, filename_2, filename_4, filename_5, plot_name, num_images, title, table_flag=False, glafic_file_1= filename_0, glafic_file_2=filename_6, shear = shear_set, show_params=False, show_shear=False)\n",
    "\n",
    "curve_data = critcurve_plot(filename_4, filename_3, pos_output=obs_data, num_images=num_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
